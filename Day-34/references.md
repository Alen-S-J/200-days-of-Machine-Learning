1. **Book:** "Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville - [Deep Learning Book](http://www.deeplearningbook.org/)

2. **Paper:** "Rectified Linear Units Improve Restricted Boltzmann Machines" by Vinod Nair and Geoffrey E. Hinton - [Read the Paper](https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf)

3. **Paper:** "Understanding the difficulty of training deep feedforward neural networks" by Xavier Glorot and Yoshua Bengio - [Read the Paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

4. **Paper:** "A Gentle Introduction to Neural Networks and Backpropagation" by Sebastien M. Popoff - [Read the Paper](https://sebastian.popovff.com/2019-03-13-a-gentle-intro-to-neural-networks-and-backpropagation/)

5. **Blog Post:** "A Gentle Introduction to the Rectified Linear Unit (ReLU)" by Jason Brownlee - [Read the Blog Post](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)

6. **Blog Post:** "Understanding Activation Functions in Neural Networks" by Disha Sharma - [Read the Blog Post](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)

7. **Video Lecture:** "Activation Functions" by Andrew Ng on Coursera - [Watch the Video](https://www.coursera.org/lecture/deep-neural-networks/activation-functions-0vBk7)

8. **GitHub Repository:** "Activation-Functions" by PyTorch - [Activation Functions Repository](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py)
