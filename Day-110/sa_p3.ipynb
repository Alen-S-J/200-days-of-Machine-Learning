{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Movie Reviews pt.3 -- n-gram\n",
    "\n",
    "## N-gram\n",
    "Text preprocessing step, we tokenized the words in reviews one by one. For example, *'Very boring movie'* will be tokenized as *\\['very','boring','movie'\\]*.\n",
    "\n",
    "This kind of model is called unigram model, because we are only taking one token at a time. However, there are other ways of tokenizing the words in n-gram model. We can instead take a sequence of tokens at a time. For example, in a bigram (2-gram) model, *'Very boring movie'* will be tokenized as *\\['very boring','boring movie'\\]*.\n",
    "\n",
    "In a trigram (3-gram) model, *'Very boring movie'* will be tokenized as a single token *'very boring movie'*.\n",
    "\n",
    "N-gram model is helpful in our sentiment analysis because sequences of words may contain more important semantics for classification. For example, the unigram 'very' does not contain any sentiment per se. 'boring' means that the reviews hates the movie. However, 'very boring' conveys that the reviewer really hates this movie, more the just 'boring'. 'very boring' shall be treated differently as 'boring' because it contains a stronger sentiment. Therefore, we need to find good n-gram models to do sentiment analysis.\n",
    "\n",
    "## Tuning parameter\n",
    "In Scikit-learn's TfidfVectorizer, we can choose the n-gram model by passing in the parameters, tuples of minimum n and maximum n. For example, (1,1) means that we are only using unigram model, since minimum n and maximum n are both 1. (1,3) means that we are using unigram, bigram, and trigram model together. For example, *'Very boring movie'* will be tokenized as *\\['very','boring','movie','very boring','boring movie','very boring movie'\\]*\n",
    "\n",
    "Therefore, we can refine the preprocess and classify function in part.1 as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer    # stem the words\n",
    "from nltk.tokenize import word_tokenize # tokenize the sentences into tokens\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # vectorize the texts\n",
    "from sklearn.model_selection import train_test_split # split the testing and training sets\n",
    "\n",
    "def preprocess(path, ngram):\n",
    "    '''generate cleaned dataset\n",
    "    \n",
    "    Args:\n",
    "        path(string): the path of the file of testing data\n",
    "        ngram(tuple (min_n, max_n)): the range of n-gram model\n",
    "\n",
    "    Returns:\n",
    "        X_train (list): the list of features of training data\n",
    "        X_test (list): the list of features of test data\n",
    "        y_train (list): the list of targets of training data ('1' or '0')\n",
    "        y_test (list): the list of targets of training data ('1' or '0')\n",
    "    '''\n",
    "    \n",
    "    # text preprocessing: iterate through the original file and \n",
    "    with open(path, encoding='utf-8') as file:\n",
    "        # record all words and its label\n",
    "        labels = []\n",
    "        preprocessed = []\n",
    "        for line in file:\n",
    "            # get sentence and label\n",
    "            sentence, label = line.strip('\\n').split('\\t')\n",
    "            labels.append(int(label))\n",
    "            \n",
    "            # remove punctuation and numbers\n",
    "            for ch in punctuation+'0123456789':\n",
    "                sentence = sentence.replace(ch,' ')\n",
    "            # tokenize the words and stem them\n",
    "            words = []\n",
    "            for w in word_tokenize(sentence):\n",
    "                words.append(PorterStemmer().stem(w))\n",
    "            preprocessed.append(' '.join(words))\n",
    "    \n",
    "    # vectorize the texts\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', sublinear_tf=True, ngram_range=ngram)\n",
    "    X = vectorizer.fit_transform(preprocessed)\n",
    "    # split the testing and training sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "def classify(clf, todense=False):\n",
    "    '''to classify the data using machine learning models\n",
    "    \n",
    "    Args:\n",
    "        clf: the model chosen to analyze the data\n",
    "        todense(bool): whether to make the sparse matrix dense\n",
    "        \n",
    "    '''\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "Because, from part.1, Multinomial Naive Bayes classifer was fast and accurate. We are going to use MultinomialNB as a baseline model for tune the parameters for it. We can pass in different tuples of parameters, from (1,1) to (3,3) to the classifer and record the performance in a Pandas dataframe as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(1, 1)</th>\n",
       "      <th>(1, 2)</th>\n",
       "      <th>(1, 3)</th>\n",
       "      <th>(2, 2)</th>\n",
       "      <th>(2, 3)</th>\n",
       "      <th>(3, 3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.795</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (1, 1)  (1, 2)  (1, 3)  (2, 2)  (2, 3)  (3, 3)\n",
       "0   0.795    0.81    0.79   0.615    0.58     0.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "# create a dictionary to record the accuracy for each ngram_range\n",
    "d = {}\n",
    "# iterate through each ngram_range\n",
    "for ngram in [(1,1),(1,2),(1,3),(2,2),(2,3),(3,3)]:\n",
    "    X_train, X_test, y_train, y_test = preprocess('imdb_labelled.txt',ngram)\n",
    "    d[str(ngram)] = [classify(MultinomialNB())]\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we must include unigram because (1,1), (1,2) and (1,3) achieve great results. (2,2)'s performance is mediocre. (2,3), (3,3)'s accuracy rate are down to 0.5, meaning that they are useless.\n",
    "\n",
    "## Smoothing\n",
    "In MultinomialNB model, we can tune the smoothing parameter $\\alpha$ of Laplace smoothing to explore a better result. For a more detailed introduction about laplace smoothing, please refer to this [article](https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece). We can choose $\\alpha$ from the list \\[0.1,0.5,1,1.5,2,2.5\\] and the n-gram model from (1,1),(1,2),(1,3). Then, run the sentiment analysis and record the accuracy in a Pandas dataframe. In this way, we can find the best pair of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>(1, 1)</th>\n",
       "      <th>(1, 2)</th>\n",
       "      <th>(1, 3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  (1, 1)  (1, 2)  (1, 3)\n",
       "0    0.1   0.815   0.785   0.780\n",
       "1    0.5   0.775   0.785   0.835\n",
       "2    1.0   0.760   0.815   0.825\n",
       "3    1.5   0.850   0.765   0.795\n",
       "4    2.0   0.805   0.765   0.795\n",
       "5    2.5   0.805   0.745   0.800"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_list = [0.1,0.5,1,1.5,2,2.5]\n",
    "d = {'alpha':alpha_list}\n",
    "for ngram in [(1,1),(1,2),(1,3)]:\n",
    "    acc = []\n",
    "    for value in alpha_list:\n",
    "        X_train, X_test, y_train, y_test = preprocess('imdb_labelled.txt',ngram)\n",
    "        acc.append(classify(MultinomialNB(alpha = value)))\n",
    "    d[ngram] = acc\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
