{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pylab as plt\n","from math import ceil\n","import numpy as np\n","from keras.models import Sequential, Model\n","from keras.layers import Input, ReLU, LeakyReLU, Dense, Activation, Reshape, Flatten\n","from keras.layers import Conv2D, Conv2DTranspose\n","from keras.optimizers import SGD, Adam\n","from keras.datasets import cifar10\n","from keras import initializers\n","from keras.layers import BatchNormalization\n"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["def build_cifar10_discriminator(ndf=64, image_shape=(32, 32, 3)):\n","    \"\"\" Builds CIFAR10 DCGAN Discriminator Model\n","    PARAMS\n","    ------\n","    ndf: number of discriminator filters\n","    image_shape: 32x32x3\n","    RETURN\n","    ------\n","    D: keras sequential\n","    \"\"\"\n","    init = initializers.RandomNormal(stddev=0.02)\n","\n","    D = Sequential()\n","\n","    # Conv 1: 16x16x64\n","    D.add(Conv2D(ndf, kernel_size=5, strides=2, padding='same',\n","                 use_bias=True, kernel_initializer=init,\n","                 input_shape=image_shape))\n","    D.add(LeakyReLU(0.2))\n","\n","    # Conv 2: 8x8x128\n","    D.add(Conv2D(ndf*2, kernel_size=5, strides=2, padding='same',\n","          use_bias=True, kernel_initializer=init))\n","    D.add(BatchNormalization())\n","    D.add(LeakyReLU(0.2))\n","\n","    # Conv 3: 4x4x256\n","    D.add(Conv2D(ndf*4, kernel_size=5, strides=2, padding='same',\n","                 use_bias=True, kernel_initializer=init))\n","    D.add(BatchNormalization())\n","    D.add(LeakyReLU(0.2))\n","\n","    # Conv 4:  2x2x512\n","    D.add(Conv2D(ndf*8, kernel_size=5, strides=2, padding='same',\n","                 use_bias=True, kernel_initializer=init))\n","    D.add(BatchNormalization())\n","    D.add(LeakyReLU(0.2))\n","\n","    # Flatten: 2x2x512 -> (2048)\n","    D.add(Flatten())\n","\n","    # Dense Layer\n","    D.add(Dense(1, kernel_initializer=init))\n","    D.add(Activation('sigmoid'))\n","\n","    print(\"\\nDiscriminator\")\n","    D.summary()\n","\n","    return D"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["def build_cifar10_generator(ngf=64, z_dim=128):\n","    \"\"\" Builds CIFAR10 DCGAN Generator Model\n","    PARAMS\n","    ------\n","    ngf: number of generator filters\n","    z_dim: number of dimensions in latent vector\n","    RETURN\n","    ------\n","    G: keras sequential\n","    \"\"\"\n","    init = initializers.RandomNormal(stddev=0.02)\n","\n","    G = Sequential()\n","\n","    # Dense 1: 2x2x512\n","    G.add(Dense(2*2*ngf*8, input_shape=(z_dim, ),\n","        use_bias=True, kernel_initializer=init))\n","    G.add(Reshape((2, 2, ngf*8)))\n","    G.add(BatchNormalization())\n","    G.add(LeakyReLU(0.2))\n","\n","    # Conv 1: 4x4x256\n","    G.add(Conv2DTranspose(ngf*4, kernel_size=5, strides=2, padding='same',\n","          use_bias=True, kernel_initializer=init))\n","    G.add(BatchNormalization())\n","    G.add(LeakyReLU(0.2))\n","\n","    # Conv 2: 8x8x128\n","    G.add(Conv2DTranspose(ngf*2, kernel_size=5, strides=2, padding='same',\n","          use_bias=True, kernel_initializer=init))\n","    G.add(BatchNormalization())\n","    G.add(LeakyReLU(0.2))\n","\n","    # Conv 3: 16x16x64\n","    G.add(Conv2DTranspose(ngf, kernel_size=5, strides=2, padding='same',\n","          use_bias=True, kernel_initializer=init))\n","    G.add(BatchNormalization())\n","    G.add(LeakyReLU(0.2))\n","\n","    # Conv 4: 32x32x3\n","    G.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same',\n","          use_bias=True, kernel_initializer=init))\n","    G.add(Activation('tanh'))\n","\n","    print(\"\\nGenerator\")\n","    G.summary()\n","\n","    return G"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["def get_data():\n","    # load cifar10 data\n","    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","    # convert train and test data to float32\n","    X_train = X_train.astype(np.float32)\n","    X_test = X_test.astype(np.float32)\n","\n","    # scale train and test data to [-1, 1]\n","    X_train = (X_train / 255) * 2 - 1\n","    X_test = (X_train / 255) * 2 - 1\n","\n","    return X_train, X_test\n","\n","\n","def plot_images(images, filename):\n","    h, w, c = images.shape[1:]\n","    images = (1/(2*2.25)) * images + 0.5\n","    grid_size = ceil(np.sqrt(images.shape[0]))\n","    images = (images.reshape(grid_size, grid_size, h, w, c)\n","              .transpose(0, 2, 1, 3, 4)\n","              .reshape(grid_size*h, grid_size*w, c))\n","    plt.figure(figsize=(16, 16))\n","    plt.imsave(filename, images)\n","\n","\n","def plot_losses(losses_d, losses_g, filename):\n","    fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n","    axes[0].plot(losses_d)\n","    axes[1].plot(losses_g)\n","    axes[0].set_title(\"losses_d\")\n","    axes[1].set_title(\"losses_g\")\n","    plt.tight_layout()\n","    plt.savefig(filename)\n","    plt.close()"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["image shape (32, 32, 3), min val -1.0, max val 1.0\n","\n","Discriminator\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 16, 16, 64)        4864      \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 16, 16, 64)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 8, 8, 128)         204928    \n","                                                                 \n"," batch_normalization (Batch  (None, 8, 8, 128)         512       \n"," Normalization)                                                  \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 4, 4, 256)         819456    \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 4, 4, 256)         1024      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 256)         0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 2, 2, 512)         3277312   \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 2, 2, 512)         2048      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 2048)              0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 2049      \n","                                                                 \n"," activation (Activation)     (None, 1)                 0         \n","                                                                 \n","=================================================================\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Total params: 4312193 (16.45 MB)\n","Trainable params: 4310401 (16.44 MB)\n","Non-trainable params: 1792 (7.00 KB)\n","_________________________________________________________________\n","\n","Generator\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_1 (Dense)             (None, 2048)              206848    \n","                                                                 \n"," reshape (Reshape)           (None, 2, 2, 512)         0         \n","                                                                 \n"," batch_normalization_3 (Bat  (None, 2, 2, 512)         2048      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 2, 2, 512)         0         \n","                                                                 \n"," conv2d_transpose (Conv2DTr  (None, 4, 4, 256)         3277056   \n"," anspose)                                                        \n","                                                                 \n"," batch_normalization_4 (Bat  (None, 4, 4, 256)         1024      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_5 (LeakyReLU)   (None, 4, 4, 256)         0         \n","                                                                 \n"," conv2d_transpose_1 (Conv2D  (None, 8, 8, 128)         819328    \n"," Transpose)                                                      \n","                                                                 \n"," batch_normalization_5 (Bat  (None, 8, 8, 128)         512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 128)         0         \n","                                                                 \n"," conv2d_transpose_2 (Conv2D  (None, 16, 16, 64)        204864    \n"," Transpose)                                                      \n","                                                                 \n"," batch_normalization_6 (Bat  (None, 16, 16, 64)        256       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n","                                                                 \n"," conv2d_transpose_3 (Conv2D  (None, 32, 32, 3)         4803      \n"," Transpose)                                                      \n","                                                                 \n"," activation_1 (Activation)   (None, 32, 32, 3)         0         \n","                                                                 \n","=================================================================\n","Total params: 4516739 (17.23 MB)\n","Trainable params: 4514819 (17.22 MB)\n","Non-trainable params: 1920 (7.50 KB)\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0\n","loss_d=0.00004, loss_g=0.00000\n","2/2 [==============================] - 2s 65ms/step\n","\tPlotting images and losses\n","Epoch 1\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[13], line 73\u001b[0m\n\u001b[0;32m     69\u001b[0m             plot_images(fake_images, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake_images_e\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n\u001b[0;32m     70\u001b[0m             plot_losses(losses_d, losses_g, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[13], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(ndf, ngf, z_dim, lr_d, lr_g, epochs, batch_size, epoch_per_checkpoint, n_checkpoint_images)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Get real samples\u001b[39;00m\n\u001b[0;32m     47\u001b[0m real_images \u001b[38;5;241m=\u001b[39m X_train[i\u001b[38;5;241m*\u001b[39mbatch_size:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size]\n\u001b[1;32m---> 48\u001b[0m loss_d_real \u001b[38;5;241m=\u001b[39m \u001b[43mD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels_real\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Fake Samples\u001b[39;00m\n\u001b[0;32m     51\u001b[0m z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(batch_size, z_dim))\n","File \u001b[1;32mc:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:2763\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2759\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2760\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2761\u001b[0m     )\n\u001b[0;32m   2762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2763\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2765\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n","File \u001b[1;32mc:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mc:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[1;32mc:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[1;32mc:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[1;32mc:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n","File \u001b[1;32mc:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def train(ndf=64, ngf=64, z_dim=100, lr_d=2e-4, lr_g=2e-4, epochs=100,\n","          batch_size=128, epoch_per_checkpoint=1, n_checkpoint_images=36):\n","\n","    X_train, _ = get_data()\n","    image_shape = X_train[0].shape\n","    print(\"image shape {}, min val {}, max val {}\".format(\n","        image_shape, X_train[0].min(), X_train[0].max()))\n","\n","    # plot real images for reference\n","    plot_images(X_train[:n_checkpoint_images], \"real_images.png\")\n","\n","    # build models\n","    D = build_cifar10_discriminator(ndf, image_shape)\n","    G = build_cifar10_generator(ngf, z_dim)\n","\n","    # define Discriminator's optimizer\n","    D.compile(Adam(lr=lr_d, beta_1=0.5), loss='binary_crossentropy',\n","              metrics=['binary_accuracy'])\n","\n","    # define D(G(z)) graph for training the Generator\n","    D.trainable = False\n","    z = Input(shape=(z_dim, ))\n","    D_of_G = Model(inputs=z, outputs=D(G(z)))\n","\n","    # define Generator's Optimizer\n","    D_of_G.compile(Adam(lr=lr_g, beta_1=0.5), loss='binary_crossentropy',\n","                   metrics=['binary_accuracy'])\n","\n","    # get labels for computing the losses\n","    labels_real = np.ones(shape=(batch_size, 1))\n","    labels_fake = np.zeros(shape=(batch_size, 1))\n","\n","    losses_d, losses_g = [], []\n","\n","    # fix a z vector for training evaluation\n","    z_fixed = np.random.uniform(-1, 1, size=(n_checkpoint_images, z_dim))\n","\n","    # training loop\n","    for e in range(epochs):\n","        print(\"Epoch {}\".format(e))\n","        for i in range(len(X_train) // batch_size):\n","\n","            # update Discriminator weights\n","            D.trainable = True\n","\n","            # Get real samples\n","            real_images = X_train[i*batch_size:(i+1)*batch_size]\n","            loss_d_real = D.train_on_batch(x=real_images, y=labels_real)[0]\n","\n","            # Fake Samples\n","            z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n","            fake_images = G.predict_on_batch(z)\n","            loss_d_fake = D.train_on_batch(x=fake_images, y=labels_fake)[0]\n","\n","            # Compute Discriminator's loss\n","            loss_d = 0.5 * (loss_d_real + loss_d_fake)\n","\n","            # update Generator weights, do not update Discriminator weights\n","            D.trainable = False\n","            loss_g = D_of_G.train_on_batch(x=z, y=labels_real)[0]\n","\n","        losses_d.append(loss_d)\n","        losses_g.append(loss_g)\n","\n","        if (e % epoch_per_checkpoint) == 0:\n","            print(\"loss_d={:.5f}, loss_g={:.5f}\".format(loss_d, loss_g))\n","            fake_images = G.predict(z_fixed)\n","            print(\"\\tPlotting images and losses\")\n","            plot_images(fake_images, \"fake_images_e{}.png\".format(e))\n","            plot_losses(losses_d, losses_g, \"losses.png\")\n","\n","\n","train()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
