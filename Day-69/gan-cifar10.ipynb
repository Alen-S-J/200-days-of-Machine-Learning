{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pylab as plt\n","from math import ceil\n","import numpy as np\n","from keras.models import Sequential, Model\n","from keras.layers import Input, ReLU, LeakyReLU, Dense, Activation, Reshape, Flatten\n","from keras.layers import Conv2D, Conv2DTranspose\n","from keras.optimizers import SGD, Adam\n","from keras.datasets import cifar10\n","from keras import initializers\n","from keras.layers import BatchNormalization\n"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["def build_cifar10_discriminator(ndf=64, image_shape=(32, 32, 3)):\n","    \"\"\" Builds CIFAR10 DCGAN Discriminator Model\n","    PARAMS\n","    ------\n","    ndf: number of discriminator filters\n","    image_shape: 32x32x3\n","    RETURN\n","    ------\n","    D: keras sequential\n","    \"\"\"\n","    init = initializers.RandomNormal(stddev=0.02)\n","\n","    D = Sequential()\n","\n","    # Conv 1: 16x16x64\n","    D.add(Conv2D(ndf, kernel_size=5, strides=2, padding='same',\n","                 use_bias=True, kernel_initializer=init,\n","                 input_shape=image_shape))\n","    D.add(LeakyReLU(0.2))\n","\n","    # Conv 2: 8x8x128\n","    D.add(Conv2D(ndf*2, kernel_size=5, strides=2, padding='same',\n","          use_bias=True, kernel_initializer=init))\n","    D.add(BatchNormalization())\n","    D.add(LeakyReLU(0.2))\n","\n","    # Conv 3: 4x4x256\n","    D.add(Conv2D(ndf*4, kernel_size=5, strides=2, padding='same',\n","                 use_bias=True, kernel_initializer=init))\n","    D.add(BatchNormalization())\n","    D.add(LeakyReLU(0.2))\n","\n","    # Conv 4:  2x2x512\n","    D.add(Conv2D(ndf*8, kernel_size=5, strides=2, padding='same',\n","                 use_bias=True, kernel_initializer=init))\n","    D.add(BatchNormalization())\n","    D.add(LeakyReLU(0.2))\n","\n","    # Flatten: 2x2x512 -> (2048)\n","    D.add(Flatten())\n","\n","    # Dense Layer\n","    D.add(Dense(1, kernel_initializer=init))\n","    D.add(Activation('sigmoid'))\n","\n","    print(\"\\nDiscriminator\")\n","    D.summary()\n","\n","    return D"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["def build_cifar10_generator(ngf=64, z_dim=128):\n","    \"\"\" Builds CIFAR10 DCGAN Generator Model\n","    PARAMS\n","    ------\n","    ngf: number of generator filters\n","    z_dim: number of dimensions in latent vector\n","    RETURN\n","    ------\n","    G: keras sequential\n","    \"\"\"\n","    init = initializers.RandomNormal(stddev=0.02)\n","\n","    G = Sequential()\n","\n","    # Dense 1: 2x2x512\n","    G.add(Dense(2*2*ngf*8, input_shape=(z_dim, ),\n","        use_bias=True, kernel_initializer=init))\n","    G.add(Reshape((2, 2, ngf*8)))\n","    G.add(BatchNormalization())\n","    G.add(LeakyReLU(0.2))\n","\n","    # Conv 1: 4x4x256\n","    G.add(Conv2DTranspose(ngf*4, kernel_size=5, strides=2, padding='same',\n","          use_bias=True, kernel_initializer=init))\n","    G.add(BatchNormalization())\n","    G.add(LeakyReLU(0.2))\n","\n","    # Conv 2: 8x8x128\n","    G.add(Conv2DTranspose(ngf*2, kernel_size=5, strides=2, padding='same',\n","          use_bias=True, kernel_initializer=init))\n","    G.add(BatchNormalization())\n","    G.add(LeakyReLU(0.2))\n","\n","    # Conv 3: 16x16x64\n","    G.add(Conv2DTranspose(ngf, kernel_size=5, strides=2, padding='same',\n","          use_bias=True, kernel_initializer=init))\n","    G.add(BatchNormalization())\n","    G.add(LeakyReLU(0.2))\n","\n","    # Conv 4: 32x32x3\n","    G.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same',\n","          use_bias=True, kernel_initializer=init))\n","    G.add(Activation('tanh'))\n","\n","    print(\"\\nGenerator\")\n","    G.summary()\n","\n","    return G"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["def get_data():\n","    # load cifar10 data\n","    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","    # convert train and test data to float32\n","    X_train = X_train.astype(np.float32)\n","    X_test = X_test.astype(np.float32)\n","\n","    # scale train and test data to [-1, 1]\n","    X_train = (X_train / 255) * 2 - 1\n","    X_test = (X_train / 255) * 2 - 1\n","\n","    return X_train, X_test\n","\n","\n","def plot_images(images, filename):\n","    h, w, c = images.shape[1:]\n","    images = (1/(2*2.25)) * images + 0.5\n","    grid_size = ceil(np.sqrt(images.shape[0]))\n","    images = (images.reshape(grid_size, grid_size, h, w, c)\n","              .transpose(0, 2, 1, 3, 4)\n","              .reshape(grid_size*h, grid_size*w, c))\n","    plt.figure(figsize=(16, 16))\n","    plt.imsave(filename, images)\n","\n","\n","def plot_losses(losses_d, losses_g, filename):\n","    fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n","    axes[0].plot(losses_d)\n","    axes[1].plot(losses_g)\n","    axes[0].set_title(\"losses_d\")\n","    axes[1].set_title(\"losses_g\")\n","    plt.tight_layout()\n","    plt.savefig(filename)\n","    plt.close()"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["image shape (32, 32, 3), min val -1.0, max val 1.0\n","\n","Discriminator\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 16, 16, 64)        4864      \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 16, 16, 64)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 8, 8, 128)         204928    \n","                                                                 \n"," batch_normalization (Batch  (None, 8, 8, 128)         512       \n"," Normalization)                                                  \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 4, 4, 256)         819456    \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 4, 4, 256)         1024      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 256)         0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 2, 2, 512)         3277312   \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 2, 2, 512)         2048      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 2048)              0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 2049      \n","                                                                 \n"," activation (Activation)     (None, 1)                 0         \n","                                                                 \n","=================================================================\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alans\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Total params: 4312193 (16.45 MB)\n","Trainable params: 4310401 (16.44 MB)\n","Non-trainable params: 1792 (7.00 KB)\n","_________________________________________________________________\n","\n","Generator\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_1 (Dense)             (None, 2048)              206848    \n","                                                                 \n"," reshape (Reshape)           (None, 2, 2, 512)         0         \n","                                                                 \n"," batch_normalization_3 (Bat  (None, 2, 2, 512)         2048      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 2, 2, 512)         0         \n","                                                                 \n"," conv2d_transpose (Conv2DTr  (None, 4, 4, 256)         3277056   \n"," anspose)                                                        \n","                                                                 \n"," batch_normalization_4 (Bat  (None, 4, 4, 256)         1024      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_5 (LeakyReLU)   (None, 4, 4, 256)         0         \n","                                                                 \n"," conv2d_transpose_1 (Conv2D  (None, 8, 8, 128)         819328    \n"," Transpose)                                                      \n","                                                                 \n"," batch_normalization_5 (Bat  (None, 8, 8, 128)         512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 128)         0         \n","                                                                 \n"," conv2d_transpose_2 (Conv2D  (None, 16, 16, 64)        204864    \n"," Transpose)                                                      \n","                                                                 \n"," batch_normalization_6 (Bat  (None, 16, 16, 64)        256       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 64)        0         \n","                                                                 \n"," conv2d_transpose_3 (Conv2D  (None, 32, 32, 3)         4803      \n"," Transpose)                                                      \n","                                                                 \n"," activation_1 (Activation)   (None, 32, 32, 3)         0         \n","                                                                 \n","=================================================================\n","Total params: 4516739 (17.23 MB)\n","Trainable params: 4514819 (17.22 MB)\n","Non-trainable params: 1920 (7.50 KB)\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0\n"]}],"source":["def train(ndf=64, ngf=64, z_dim=100, lr_d=2e-4, lr_g=2e-4, epochs=100,\n","          batch_size=128, epoch_per_checkpoint=1, n_checkpoint_images=36):\n","\n","    X_train, _ = get_data()\n","    image_shape = X_train[0].shape\n","    print(\"image shape {}, min val {}, max val {}\".format(\n","        image_shape, X_train[0].min(), X_train[0].max()))\n","\n","    # plot real images for reference\n","    plot_images(X_train[:n_checkpoint_images], \"real_images.png\")\n","\n","    # build models\n","    D = build_cifar10_discriminator(ndf, image_shape)\n","    G = build_cifar10_generator(ngf, z_dim)\n","\n","    # define Discriminator's optimizer\n","    D.compile(Adam(lr=lr_d, beta_1=0.5), loss='binary_crossentropy',\n","              metrics=['binary_accuracy'])\n","\n","    # define D(G(z)) graph for training the Generator\n","    D.trainable = False\n","    z = Input(shape=(z_dim, ))\n","    D_of_G = Model(inputs=z, outputs=D(G(z)))\n","\n","    # define Generator's Optimizer\n","    D_of_G.compile(Adam(lr=lr_g, beta_1=0.5), loss='binary_crossentropy',\n","                   metrics=['binary_accuracy'])\n","\n","    # get labels for computing the losses\n","    labels_real = np.ones(shape=(batch_size, 1))\n","    labels_fake = np.zeros(shape=(batch_size, 1))\n","\n","    losses_d, losses_g = [], []\n","\n","    # fix a z vector for training evaluation\n","    z_fixed = np.random.uniform(-1, 1, size=(n_checkpoint_images, z_dim))\n","\n","    # training loop\n","    for e in range(epochs):\n","        print(\"Epoch {}\".format(e))\n","        for i in range(len(X_train) // batch_size):\n","\n","            # update Discriminator weights\n","            D.trainable = True\n","\n","            # Get real samples\n","            real_images = X_train[i*batch_size:(i+1)*batch_size]\n","            loss_d_real = D.train_on_batch(x=real_images, y=labels_real)[0]\n","\n","            # Fake Samples\n","            z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n","            fake_images = G.predict_on_batch(z)\n","            loss_d_fake = D.train_on_batch(x=fake_images, y=labels_fake)[0]\n","\n","            # Compute Discriminator's loss\n","            loss_d = 0.5 * (loss_d_real + loss_d_fake)\n","\n","            # update Generator weights, do not update Discriminator weights\n","            D.trainable = False\n","            loss_g = D_of_G.train_on_batch(x=z, y=labels_real)[0]\n","\n","        losses_d.append(loss_d)\n","        losses_g.append(loss_g)\n","\n","        if (e % epoch_per_checkpoint) == 0:\n","            print(\"loss_d={:.5f}, loss_g={:.5f}\".format(loss_d, loss_g))\n","            fake_images = G.predict(z_fixed)\n","            print(\"\\tPlotting images and losses\")\n","            plot_images(fake_images, \"fake_images_e{}.png\".format(e))\n","            plot_losses(losses_d, losses_g, \"losses.png\")\n","\n","\n","train()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
