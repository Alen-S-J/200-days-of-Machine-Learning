

### Example 1: Simple Linear Regression

Let's consider a simple linear regression with one independent variable \( x \) and one dependent variable \( y \). The mathematical representation of this relationship is given by:

\[ y = 2x + 3 + \varepsilon \]

In this example:
- \( y \) is the dependent variable we want to predict.
- \( x \) is the independent variable.
- The slope \( \beta_1 \) is 2, representing the change in \( y \) for a one-unit change in \( x \).
- The y-intercept \( \beta_0 \) is 3, representing the value of \( y \) when \( x \) is 0.
- \( \varepsilon \) is the error term representing the difference between the observed \( y \) and the predicted \( y \).

### Example 2: Multiple Linear Regression

In multiple linear regression, we have more than one independent variable. The mathematical representation is extended to accommodate multiple variables. For instance:

\[ y = 3x_1 + 5x_2 - 2x_3 + 7 + \varepsilon \]

In this example:
- \( y \) is the dependent variable.
- \( x_1, x_2, x_3 \) are the independent variables.
- \( \beta_1 = 3 \), \( \beta_2 = 5 \), and \( \beta_3 = -2 \) are the respective slopes for each independent variable.
- \( \beta_0 = 7 \) is the y-intercept.
- \( \varepsilon \) is the error term.

These examples demonstrate the mathematical representation of linear regression, showing the relationship between the dependent variable and the independent variable(s).